{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Using tf-idf and random forests"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Newsgroups Data"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We will take a look at some of the twenty newsgroups dataset, another common dataset for classification. Note that the data is fetched from."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.datasets import fetch_20newsgroups\n",
      "\n",
      "# We will use four of the twenty newsgroups\n",
      "categories = ['alt.atheism',\n",
      "              'talk.religion.misc',\n",
      "              'comp.graphics',\n",
      "              'sci.space']\n",
      "\n",
      "twenty_train_subset = fetch_20newsgroups(subset='train', categories=categories)\n",
      "twenty_test_subset = fetch_20newsgroups(subset='test', categories=categories)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have lists of messages (as strings) in the `.data` members."
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "tf-idf features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Here are some ways to generate features from the text:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "\n",
      "# Turn the text documents into vectors of word frequencies\n",
      "vectorizer = CountVectorizer()\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This makes a matrix of word counts, where each row is a document and each column is the word, the cell matrix[document, word] contains the count of word in document.\n",
      "\n",
      "We can expand on this by setting the ngram_range. This parameter allows us to set each column not only as one word, but possibly sequences of words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Include every 1-gram, 2-gram, and 3-gram\n",
      "vectorizer = CountVectorizer(ngram_range=(1,3))\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Additionally, we could use a tf-idf representation, which stands for Term Frequency - Inverse Document Frequency.\n",
      "\n",
      "This value is the product of two intermediate values, the Term Frequency and the Inverse Document Frequency.\n",
      "\n",
      "The Term Frequency is equivalent to the CountVectorizer features, the number of times or count that a word appear in the document. This is our most basic representation of text.\n",
      "\n",
      "To establish Inverse Document Frequency, first let's define Document Frequency. This is the percentage of documents that a particular word appears in. For example, the word `the` might appear in 100% of documents, while words like `Syria` would likely have low document frequency. Inverse Document Frequency is simply 1 / Document Frequency (although often the log is also taken).\n",
      "\n",
      "So tf-idf is Term Frequency * Inverse Document Frequency, or similar to Term Frequency / Document Frequency. The intuition is that words that have high weight are those that appear a lot in this document and/or appear in very few other documents (somehow unique to this document)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = TfidfVectorizer()\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can put this together with our other tricks as well."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,5))\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Random Forests"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Random Forests are very popular ensemble classifiers. They are relatively simple to use (very few parameters to set and easy to avoid overfitting). The only parameter we are really worried about is the number of trees we want to create - n_estimators in sklearn."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.ensemble import RandomForestClassifier\n",
      "\n",
      "model = RandomForestClassifier(n_estimators=10)\n",
      "#model.fit(...) # as usual"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can use predict using our 20-newsgroup dataset above"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.cross_validation import cross_val_score\n",
      "from sklearn.tree import DecisionTreeClassifier\n",
      "\n",
      "vectorizer = TfidfVectorizer(stop_words='english')\n",
      "X_train = vectorizer.fit_transform(twenty_train_subset.data)\n",
      "\n",
      "tree_model = DecisionTreeClassifier()\n",
      "print cross_val_score(tree_model, X_train.toarray(), twenty_train_subset.target)\n",
      "\n",
      "rf_model = RandomForestClassifier(n_estimators=20)\n",
      "print cross_val_score(rf_model, X_train.toarray(), twenty_train_subset.target)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Getting Important Features"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Random Forests can quantify the importance of features. Unlike with logistic regression, we don't have coeffcients that tell us relative impact. But we can keep track of what features give us the best splits."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "rf_model = RandomForestClassifier(n_estimators=10, compute_importances=True)\n",
      "rf_model.fit(X_train.toarray(), twenty_train_subset.target)\n",
      "\n",
      "# This prints the top 10 most important features\n",
      "print sorted(zip(rf_model.feature_importances_, vectorizer.get_feature_names()), reverse=True)[:20]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(0.01702740436365642, u'atheists'), (0.015802879900491687, u'space'), (0.011151143651623625, u'orbit'), (0.010912617836595268, u'allan'), (0.010431420570992779, u'writes'), (0.008058308977904164, u'religion'), (0.0076127862008405541, u'god'), (0.0075980214465481257, u'hi'), (0.0062369911486066772, u'moral'), (0.0060871108591429128, u'aurora'), (0.0057858145685441478, u'pat'), (0.005547962189158975, u'shuttle'), (0.0054469975561023034, u'lunar'), (0.0054034542470881913, u'keith'), (0.0047251609306518325, u'graphics'), (0.0045885529225289717, u'nasa'), (0.0043898200122270215, u'points'), (0.0043602787115481116, u'prb'), (0.0043244024858243111, u'years'), (0.0043055721417126396, u'article')]\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/Users/edjoy/anaconda/lib/python2.7/site-packages/sklearn/ensemble/forest.py:783: DeprecationWarning: Setting compute_importances is no longer required as version 0.14. Variable importances are now computed on the fly when accessing the feature_importances_ attribute. This parameter will be removed in 0.16.\n",
        "  DeprecationWarning)\n"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Extensions:\n",
      "\n",
      "* Experiment with other options to the various pieces used here.\n",
      "* Evaluate performance for various models on the hold-out test set that was initially loaded.\n",
      "* Explore AdaBoost and other methods in the sklearn.ensemble module."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}